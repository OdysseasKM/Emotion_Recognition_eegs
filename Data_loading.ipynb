{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import scipy.stats as stats\n",
    "import pyentrp.entropy as ent\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA LOADING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the eegs of the first 5 participants into eeg_data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_ids = [f\"s{i:02d}\" for i in range(1, 6)]  # Assuming 5 participants\n",
    "eeg_data = {}\n",
    "\n",
    "# Load EEG data for each participant\n",
    "for participant_id in participant_ids:\n",
    "    file_path = f\"eegs/{participant_id}.bdf\"\n",
    "    raw = mne.io.read_raw_bdf(file_path, preload=True)\n",
    "    eeg_data[participant_id] = raw\n",
    "\n",
    "# Print information about loaded data\n",
    "for participant_id, raw in eeg_data.items():\n",
    "    print(f\"Participant ID: {participant_id}\")\n",
    "    # print(raw.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the eeg channels, reorder them to Geneva order and store them to eeg_data_reordered dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "# Define the channel names and indices for Twente and Geneva\n",
    "channel_names_geneva = ['Fp1', 'AF3', 'F3', 'F7', 'FC5', 'FC1', 'C3', 'T7', 'CP5', 'CP1', 'P3', 'P7', 'PO3', 'O1', 'Oz', 'Pz', 'Fp2', 'AF4', 'Fz', 'F4', 'F8', 'FC6', 'FC2', 'Cz', 'C4', 'T8', 'CP6', 'CP2', 'P4', 'P8', 'PO4', 'O2']\n",
    "participant_ids = [f\"s{i:02d}\" for i in range(1, 6)]  # Assuming 5 participants\n",
    "eeg_data_reordered = {}\n",
    "\n",
    "# Load EEG data for each participant and reorder channels\n",
    "for participant_id in participant_ids:\n",
    "    file_path = f\"eegs/{participant_id}.bdf\"\n",
    "    raw = mne.io.read_raw_bdf(file_path, preload=True)\n",
    "    \n",
    "    # Keep only EEG channels\n",
    "    raw_eeg = raw.pick_types(eeg=True)\n",
    "    \n",
    "    # Reorder EEG channels to Geneva order\n",
    "    raw_reordered = raw_eeg.reorder_channels(channel_names_geneva)\n",
    "\n",
    "    # Store reordered EEG data\n",
    "    eeg_data_reordered[participant_id] = raw_reordered\n",
    "\n",
    "# Print information about loaded and reordered data\n",
    "for participant_id, raw_reordered in eeg_data_reordered.items():\n",
    "    print(f\"Participant ID: {participant_id}\")\n",
    "    print(\"Reordered EEG data shape:\", raw_reordered._data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check new channels and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Iterate through each participant\n",
    "for participant_id, raw_reordered in eeg_data_reordered.items():\n",
    "    print(f\"Participant ID: {participant_id}\")\n",
    "    \n",
    "    # Before reordering\n",
    "    print(\"Channel names before reordering:\", raw.info['ch_names'])\n",
    "    \n",
    "    # # Plot channel locations before reordering\n",
    "    # fig_before = raw.plot_sensors(show_names=True, title=\"Channel Locations Before Reordering\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # After reordering\n",
    "    print(\"Channel names after reordering:\", raw_reordered.info['ch_names'])\n",
    "    \n",
    "    # # Plot channel locations after reordering\n",
    "    # fig_after = raw_reordered.plot_sensors(show_names=True, title=\"Channel Locations After Reordering\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # Check data shape before and after reordering\n",
    "    print(\"EEG data shape before reordering:\", raw._data.shape)\n",
    "    print(\"EEG data shape after reordering:\", raw_reordered._data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and sort based on experiment_id the Participants_ratings csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"participant_ratings.csv\")\n",
    "df_sorted = df.sort_values(by=['Participant_id', 'Experiment_id'])\n",
    "\n",
    "df_sorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devide each eeg signal of each one of the participants into 40 60sec trials that represent the 40 experiments that took place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store trial data for each participant\n",
    "participant_trials = {participant_id: [] for participant_id in participant_ids}\n",
    "\n",
    "trial_info = df_sorted\n",
    "\n",
    "# Iterate over each participant's data for the first 5 participants\n",
    "for participant_id, participant_data in trial_info.groupby('Participant_id'):\n",
    "    participant_id_str = f\"s{participant_id:02d}\"  # Convert participant_id to string format\n",
    "    if participant_id_str not in participant_ids:\n",
    "        continue\n",
    "\n",
    "    # Get the raw EEG data for the current participant\n",
    "    raw_data = eeg_data_reordered[participant_id_str]\n",
    "\n",
    "    # Iterate over each trial for the current participant\n",
    "    for index, trial in participant_data.iterrows():\n",
    "        # Extract the start time of the trial (in seconds)\n",
    "        start_time = trial['Start_time'] / 1e6  # Convert microseconds to seconds\n",
    "\n",
    "        # Define the start and end time of the trial (assuming 1-minute duration)\n",
    "        end_time = start_time + 60  # 1-minute duration\n",
    "\n",
    "        # Extract the trial data based on the start and end time\n",
    "        trial_data = raw_data.copy().crop(tmin=start_time, tmax=end_time)\n",
    "\n",
    "        # Store the trial data in the list for the current participant\n",
    "        participant_trials[participant_id_str].append(trial_data)\n",
    "\n",
    "        # Print participant ID and trial information\n",
    "        # print(participant_id_str, trial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of participants\n",
    "num_participants = len(participant_trials)\n",
    "print(f\"Number of participants: {num_participants}\")\n",
    "\n",
    "# Get the number of trials for each participant\n",
    "for participant_id, trials in participant_trials.items():\n",
    "    num_trials = len(trials)\n",
    "    print(f\"Participant {participant_id}: Number of trials = {num_trials}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the trials of the first participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the trial information DataFrame for the first participant\n",
    "first_participant_trial_info = trial_info[trial_info['Participant_id'] == 1]\n",
    "\n",
    "# Print the information of each trial\n",
    "for index, trial_data in first_participant_trial_info.iterrows():\n",
    "    print(f\"Trial {index + 1} info:\")\n",
    "    print(trial_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the trials of the first participant into first_participant_trials and plot the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first_participant_trials = participant_trials['s01']\n",
    "first_trial_data = first_participant_trials[0]\n",
    "first_trial_data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the psd of the first trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_trial_data.plot_psd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bandpass filter 4->45 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_freq = 4  # Lower cutoff frequency in Hz\n",
    "high_freq = 45  # Upper cutoff frequency in Hz\n",
    "\n",
    "# Iterate through each trial\n",
    "for i, trial_data in enumerate(first_participant_trials):\n",
    "    # Apply band-pass filter\n",
    "    trial_data.filter(low_freq, high_freq)\n",
    "    \n",
    "    # Plot the EEG data for the first trial after filtering\n",
    "    if i == 0:\n",
    "        trial_data.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notch filter to cut off 50Hz and 60 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notch_freqs = [50, 60]  # Notch filter frequencies in Hz\n",
    "\n",
    "# Iterate through each trial\n",
    "for i, trial_data in enumerate(first_participant_trials):\n",
    "    # Apply notch filters\n",
    "    for freq in notch_freqs:\n",
    "        trial_data.notch_filter(freqs=freq, verbose=True)\n",
    "    \n",
    "    # Plot the EEG data for the first trial after applying the notch filters\n",
    "    if i == 0:\n",
    "        trial_data.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set new sampling rate to 128 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new sampling rate\n",
    "new_sampling_rate = 128\n",
    "\n",
    "# Resample each trial for the first participant\n",
    "for i, trial_data in enumerate(first_participant_trials):\n",
    "    first_participant_trials[i] = trial_data.resample(new_sampling_rate, npad=\"auto\")\n",
    "\n",
    "# Plot the EEG data for the first trial after resampling\n",
    "first_participant_trials[0].plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_participant_trials[0].ch_names)\n",
    "first_participant_trials[0].info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot psd of the preprocessed first trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_participant_trials[0].plot_psd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(first_participant_trials[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ICA to remove artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.preprocessing import ICA\n",
    "\n",
    "# Set a fractional value for n_components\n",
    "n_components_fraction = 32\n",
    "\n",
    "# Initialize ICA with the extended Infomax method\n",
    "ica = ICA(n_components=n_components_fraction, method='infomax')\n",
    "\n",
    "# Fit the ICA model to your data\n",
    "ica.fit(first_participant_trials[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot each component separately\n",
    "for i in range(len(ica.mixing_matrix_)):\n",
    "    plt.figure()\n",
    "    plt.plot(ica.mixing_matrix_[i])\n",
    "    plt.title(f\"Component {i+1}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying common average reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, trial_data in enumerate(first_participant_trials):\n",
    "    first_participant_trials[i] = trial_data.copy().set_eeg_reference('average', projection=True)\n",
    "    first_participant_trials[i].apply_proj()\n",
    "\n",
    "# Plot the EEG data for the first trial after applying CAR\n",
    "first_participant_trials[0].plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviding each trial into 60 1sec subtrials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the duration of each subtrial in seconds\n",
    "subtrial_duration = 1  # 1 second\n",
    "\n",
    "# Initialize a list to store the subtrials for all 40 trials of the first participant\n",
    "first_participant_subtrials = []\n",
    "\n",
    "# Iterate over each trial for the first participant\n",
    "for trial_data in first_participant_trials:\n",
    "    # Get the total duration of the trial in seconds\n",
    "    trial_duration = trial_data.times[-1]\n",
    "\n",
    "    # Calculate the number of subtrials\n",
    "    num_subtrials = int(np.floor(trial_duration / subtrial_duration))\n",
    "\n",
    "    # Initialize a list to store the subtrials for the current trial\n",
    "    trial_subtrials = []\n",
    "\n",
    "    # Iterate over each subtrial\n",
    "    for i in range(num_subtrials):\n",
    "        # Define the start and end time of the subtrial\n",
    "        start_time = i * subtrial_duration\n",
    "        end_time = (i + 1) * subtrial_duration\n",
    "\n",
    "        # Extract the subtrial data\n",
    "        subtrial_data = trial_data.copy().crop(tmin=start_time, tmax=end_time)\n",
    "\n",
    "        # Append the subtrial data to the list\n",
    "        trial_subtrials.append(subtrial_data)\n",
    "\n",
    "    # Append the list of subtrials for the current trial to the list for all trials\n",
    "    first_participant_subtrials.append(trial_subtrials)\n",
    "\n",
    "# Now you have a nested list where first_participant_subtrials[i][j] represents the j-th subtrial of the i-th trial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trial_subtrials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut the first 3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first three subtrials from each trial's list of subtrials for all 40 trials of the first participant\n",
    "for trial_subtrials in first_participant_subtrials:\n",
    "    trial_subtrials = trial_subtrials[2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trial_subtrials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract activity, complexity, mobility (Hjorth parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store computed Hjorth parameters for all trials of the first participant\n",
    "all_activities = []\n",
    "all_mobilities = []\n",
    "all_complexities = []\n",
    "\n",
    "# Iterate over each trial's list of subtrials\n",
    "for trial_subtrials in first_participant_subtrials:\n",
    "    # Initialize arrays to store computed Hjorth parameters for subtrials of the current trial\n",
    "    trial_activities = []\n",
    "    trial_mobilities = []\n",
    "    trial_complexities = []\n",
    "\n",
    "    # Compute Hjorth parameters for each subtrial of the current trial\n",
    "    for subtrial_data in trial_subtrials:\n",
    "        # Ensure the data is 2-dimensional\n",
    "        subtrial_data_2d = subtrial_data.get_data().squeeze()\n",
    "\n",
    "        # Compute the first derivative\n",
    "        dy = np.diff(subtrial_data_2d, axis=1)\n",
    "\n",
    "        # Compute the second derivative\n",
    "        dyy = np.diff(subtrial_data_2d, n=2, axis=1)\n",
    "\n",
    "        # Compute activity\n",
    "        activity = np.var(subtrial_data_2d, axis=1)\n",
    "\n",
    "        # Compute mobility\n",
    "        mobility = np.sqrt(np.var(dy, axis=1) / activity)\n",
    "\n",
    "        # Compute complexity\n",
    "        complexity = np.sqrt(np.var(dyy, axis=1) / np.var(dy, axis=1)) / mobility\n",
    "\n",
    "        # Append computed Hjorth parameters to respective arrays\n",
    "        trial_activities.append(activity)\n",
    "        trial_mobilities.append(mobility)\n",
    "        trial_complexities.append(complexity)\n",
    "\n",
    "    # Convert lists to arrays and append them to corresponding lists for all trials\n",
    "    all_activities.append(np.array(trial_activities))\n",
    "    all_mobilities.append(np.array(trial_mobilities))\n",
    "    all_complexities.append(np.array(trial_complexities))\n",
    "\n",
    "# Now all_activities, all_mobilities, and all_complexities contain the computed Hjorth parameters for each subtrial of all trials of the first participant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print computed Hjorth parameters for each subtrial of the first trial\n",
    "for i, (activity, mobility, complexity) in enumerate(zip(all_activities[0], all_mobilities[0], all_complexities[0]), start=1):\n",
    "    print(f\"Subtrial {i}:\")\n",
    "    print(f\"Activity: {activity}\")\n",
    "    print(f\"Mobility: {mobility}\")\n",
    "    print(f\"Complexity: {complexity}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store mean Hjorth parameters for each trial\n",
    "trial_mean_activities = []\n",
    "trial_mean_mobilities = []\n",
    "trial_mean_complexities = []\n",
    "\n",
    "# Iterate over each trial\n",
    "for trial_activities, trial_mobilities, trial_complexities in zip(all_activities, all_mobilities, all_complexities):\n",
    "    # Compute the mean across all subtrials for each Hjorth parameter individually\n",
    "    mean_activity = np.mean(trial_activities, axis=0)\n",
    "    mean_mobility = np.mean(trial_mobilities, axis=0)\n",
    "    mean_complexity = np.mean(trial_complexities, axis=0)\n",
    "    \n",
    "    # Append the mean Hjorth parameters for the current trial to corresponding lists\n",
    "    trial_mean_activities.append(mean_activity)\n",
    "    trial_mean_mobilities.append(mean_mobility)\n",
    "    trial_mean_complexities.append(mean_complexity)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "trial_mean_activities = np.array(trial_mean_activities)\n",
    "trial_mean_mobilities = np.array(trial_mean_mobilities)\n",
    "trial_mean_complexities = np.array(trial_mean_complexities)\n",
    "\n",
    "# Print the shape of the resulting arrays\n",
    "print(\"Shape of trial_mean_activities:\", trial_mean_activities.shape)\n",
    "print(\"Shape of trial_mean_mobilities:\", trial_mean_mobilities.shape)\n",
    "print(\"Shape of trial_mean_complexities:\", trial_mean_complexities.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean of the 32 values for each trial\n",
    "trial_mean_activity_final = np.mean(trial_mean_activities, axis=1)\n",
    "trial_mean_mobility_final = np.mean(trial_mean_mobilities, axis=1)\n",
    "trial_mean_complexity_final = np.mean(trial_mean_complexities, axis=1)\n",
    "\n",
    "# Print the mean of the 32 values for each trial\n",
    "for i, (mean_activity, mean_mobility, mean_complexity) in enumerate(zip(trial_mean_activity_final, trial_mean_mobility_final, trial_mean_complexity_final), start=1):\n",
    "    print(f\"Trial {i}:\")\n",
    "    print(f\"Mean Activity: {mean_activity}\")\n",
    "    print(f\"Mean Mobility: {mean_mobility}\")\n",
    "    print(f\"Mean Complexity: {mean_complexity}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "outcome_df = pd.DataFrame()\n",
    "\n",
    "# Initialize lists to store participant ID, experiment ID, and computed Hjorth parameters\n",
    "participant_id = \"1\"  # Participant ID 's01' repeated 40 times\n",
    "experiment_ids = list(range(1, 41))  # Experiment IDs from 1 to 40\n",
    "\n",
    "# Create a dictionary to hold the features\n",
    "data = {\n",
    "    \"Participant_id\": participant_id,\n",
    "    \"Experiment_id\": experiment_ids,\n",
    "    \"Activity\": trial_mean_activity_final,\n",
    "    \"Mobility\": trial_mean_mobility_final,\n",
    "    \"Complexity\": trial_mean_complexity_final\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "outcome_df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(outcome_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract statistical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the features for all trials of the first participant\n",
    "all_mean_values = []\n",
    "all_std_values = []\n",
    "all_max_values = []\n",
    "all_min_values = []\n",
    "all_rms_values = []\n",
    "all_skewness_values = []\n",
    "all_kurtosis_values = []\n",
    "all_entropy_values = []\n",
    "\n",
    "# Iterate over each trial's list of subtrials\n",
    "for trial_subtrials in first_participant_subtrials:\n",
    "    # Initialize lists to store the features for subtrials of the current trial\n",
    "    mean_values = []\n",
    "    std_values = []\n",
    "    max_values = []\n",
    "    min_values = []\n",
    "    rms_values = []\n",
    "    skewness_values = []\n",
    "    kurtosis_values = []\n",
    "    entropy_values = []\n",
    "\n",
    "    # Iterate over each subtrial of the current trial\n",
    "    for subtrial_data in trial_subtrials:\n",
    "        # Flatten the subtrial data\n",
    "        flat_data = subtrial_data.get_data().flatten()\n",
    "\n",
    "        # Compute mean\n",
    "        mean_values.append(np.mean(flat_data))\n",
    "\n",
    "        # Compute standard deviation\n",
    "        std_values.append(np.std(flat_data))\n",
    "\n",
    "        # Compute maximum and minimum values\n",
    "        max_values.append(np.max(flat_data))\n",
    "        min_values.append(np.min(flat_data))\n",
    "\n",
    "        # Compute root mean square (RMS)\n",
    "        rms_values.append(np.sqrt(np.mean(flat_data**2)))\n",
    "\n",
    "        # Compute skewness and kurtosis\n",
    "        skewness_values.append(stats.skew(flat_data))\n",
    "        kurtosis_values.append(stats.kurtosis(flat_data))\n",
    "\n",
    "        # Compute sample entropy\n",
    "        sample_entropy = ent.sample_entropy(flat_data, 2, 0.2*np.std(flat_data))\n",
    "        entropy_values.append(sample_entropy)\n",
    "\n",
    "    # Append computed features for subtrials of the current trial to corresponding lists\n",
    "    all_mean_values.append(mean_values)\n",
    "    all_std_values.append(std_values)\n",
    "    all_max_values.append(max_values)\n",
    "    all_min_values.append(min_values)\n",
    "    all_rms_values.append(rms_values)\n",
    "    all_skewness_values.append(skewness_values)\n",
    "    all_kurtosis_values.append(kurtosis_values)\n",
    "    all_entropy_values.append(entropy_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print computed features for each subtrial of each trial\n",
    "for i, (mean_trial, std_trial, max_trial, min_trial, rms_trial, skewness_trial, kurtosis_trial, entropy_trial) in enumerate(zip(all_mean_values, all_std_values, all_max_values, all_min_values, all_rms_values, all_skewness_values, all_kurtosis_values, all_entropy_values), start=1):\n",
    "    print(f\"Trial {i}:\")\n",
    "    for j, (mean_subtrial, std_subtrial, max_subtrial, min_subtrial, rms_subtrial, skewness_subtrial, kurtosis_subtrial, entropy_subtrial) in enumerate(zip(mean_trial, std_trial, max_trial, min_trial, rms_trial, skewness_trial, kurtosis_trial, entropy_trial), start=1):\n",
    "        print(f\"Subtrial {j}:\")\n",
    "        print(f\"Mean: {mean_subtrial}\")\n",
    "        print(f\"Standard Deviation: {std_subtrial}\")\n",
    "        print(f\"Maximum: {max_subtrial}\")\n",
    "        print(f\"Minimum: {min_subtrial}\")\n",
    "        print(f\"Root Mean Square: {rms_subtrial}\")\n",
    "        print(f\"Skewness: {skewness_subtrial}\")\n",
    "        print(f\"Kurtosis: {kurtosis_subtrial}\")\n",
    "        print(f\"Entropy: {entropy_subtrial}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the mean values of features for each trial\n",
    "trial_mean_values = []\n",
    "trial_std_values = []\n",
    "trial_max_values = []\n",
    "trial_min_values = []\n",
    "trial_rms_values = []\n",
    "trial_skewness_values = []\n",
    "trial_kurtosis_values = []\n",
    "trial_entropy_values = []\n",
    "\n",
    "# Iterate over each trial's features\n",
    "for trial_features in zip(all_mean_values, all_std_values, all_max_values, all_min_values, all_rms_values, all_skewness_values, all_kurtosis_values, all_entropy_values):\n",
    "    # Compute the mean value of each feature across all subtrials for the current trial\n",
    "    trial_mean_values.append(np.mean(trial_features[0]))\n",
    "    trial_std_values.append(np.mean(trial_features[1]))\n",
    "    trial_max_values.append(np.mean(trial_features[2]))\n",
    "    trial_min_values.append(np.mean(trial_features[3]))\n",
    "    trial_rms_values.append(np.mean(trial_features[4]))\n",
    "    trial_skewness_values.append(np.mean(trial_features[5]))\n",
    "    trial_kurtosis_values.append(np.mean(trial_features[6]))\n",
    "    trial_entropy_values.append(np.mean(trial_features[7]))\n",
    "\n",
    "# Print the mean values of features for each trial\n",
    "for i, (mean_val, std_val, max_val, min_val, rms_val, skewness_val, kurtosis_val, entropy_val) in enumerate(zip(trial_mean_values, trial_std_values, trial_max_values, trial_min_values, trial_rms_values, trial_skewness_values, trial_kurtosis_values, trial_entropy_values), start=1):\n",
    "    print(f\"Trial {i}:\")\n",
    "    print(f\"Mean: {mean_val}\")\n",
    "    print(f\"Standard Deviation: {std_val}\")\n",
    "    print(f\"Maximum: {max_val}\")\n",
    "    print(f\"Minimum: {min_val}\")\n",
    "    print(f\"Root Mean Square: {rms_val}\")\n",
    "    print(f\"Skewness: {skewness_val}\")\n",
    "    print(f\"Kurtosis: {kurtosis_val}\")\n",
    "    print(f\"Entropy: {entropy_val}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Create a list to hold the participant ID (s01) and experiment ID (1 to 40) for each trial\n",
    "# participant_ids = [\"s01\"] * 40\n",
    "# experiment_ids = list(range(1, 41))\n",
    "\n",
    "new_data = {\n",
    "    \"Mean\": trial_mean_values,\n",
    "    \"Standard_Deviation\": trial_std_values,\n",
    "    \"Maximum\": trial_max_values,\n",
    "    \"Minimum\": trial_min_values,\n",
    "    \"Root_Mean_Square\": trial_rms_values,\n",
    "    \"Skewness\": trial_skewness_values,\n",
    "    \"Kurtosis\": trial_kurtosis_values,\n",
    "    \"Entropy\": trial_entropy_values\n",
    "}\n",
    "\n",
    "\n",
    "# Update the existing dictionary with new features\n",
    "data.update(new_data)\n",
    "\n",
    "# Create a DataFrame from the updated dictionary\n",
    "outcome_df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(outcome_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add valence and arousal columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_data = pd.read_csv(\"participant_ratings.csv\")\n",
    "\n",
    "# Convert Participant_id column in CSV data to string type\n",
    "csv_data['Participant_id'] = csv_data['Participant_id'].astype(str)\n",
    "\n",
    "# Merge the CSV data with our DataFrame based on participant_id and experiment_id\n",
    "merged_data = pd.merge(outcome_df, csv_data[['Participant_id', 'Experiment_id', 'Valence', 'Arousal']], \n",
    "                       how='left', on=['Participant_id', 'Experiment_id'])\n",
    "\n",
    "# Print the merged DataFrame\n",
    "print(merged_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute energy and differential entropy for each band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fft import fft\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "fs = 128  \n",
    "\n",
    "def compute_energy(signal, sampling_rate, frequency_band=None):\n",
    "    \n",
    "    fft_result = fft(signal)\n",
    "    freqs = np.fft.fftfreq(len(signal), d=1/sampling_rate)\n",
    "    if frequency_band is not None:\n",
    "        band_indices = np.where((freqs >= frequency_band[0]) & (freqs <= frequency_band[1]))[0]\n",
    "        band_fft = fft_result[band_indices]\n",
    "    else:\n",
    "        band_fft = fft_result\n",
    "    energy = np.sum(np.abs(band_fft)**2) / len(signal)\n",
    "    return energy\n",
    "\n",
    "\n",
    "def compute_differential_entropy(signal, sampling_rate, frequency_band):\n",
    "    \n",
    "    fft_result = fft(signal)\n",
    "    freqs = np.fft.fftfreq(len(signal), d=1/sampling_rate)\n",
    "    band_indices = np.where((freqs >= frequency_band[0]) & (freqs <= frequency_band[1]))[0]\n",
    "    band_fft = fft_result[band_indices]\n",
    "    band_psd = np.abs(band_fft)**2 / len(signal)\n",
    "    band_pdf = band_psd / np.sum(band_psd)\n",
    "    diff_entropy = entropy(band_pdf)\n",
    "    return diff_entropy\n",
    "\n",
    "# Define frequency bands\n",
    "theta_band = (4, 8)\n",
    "alpha_band = (8, 14)\n",
    "beta_band = (14, 31)\n",
    "gamma_band = (31, 45)\n",
    "\n",
    "# Initialize lists to store energy values for each frequency band\n",
    "theta_energy_values = []\n",
    "alpha_energy_values = []\n",
    "beta_energy_values = []\n",
    "gamma_energy_values = []\n",
    "\n",
    "# Initialize lists to store differential entropy values for each frequency band\n",
    "theta_diff_entropy_values = []\n",
    "alpha_diff_entropy_values = []\n",
    "beta_diff_entropy_values = []\n",
    "gamma_diff_entropy_values = []\n",
    "t=0\n",
    "# Loop through each trial\n",
    "for trial_subtrials in first_participant_subtrials:\n",
    "    print(t)\n",
    "    t += 1\n",
    "    # Initialize lists to store energy and differential entropy for each channel\n",
    "    trial_theta_energy_values = []\n",
    "    trial_alpha_energy_values = []\n",
    "    trial_beta_energy_values = []\n",
    "    trial_gamma_energy_values = []\n",
    "\n",
    "    trial_theta_diff_entropy_values = []\n",
    "    trial_alpha_diff_entropy_values = []\n",
    "    trial_beta_diff_entropy_values = []\n",
    "    trial_gamma_diff_entropy_values = []\n",
    "\n",
    "    # Loop through each subtrial in the trial\n",
    "    for subtrial_data in trial_subtrials:\n",
    "        # Initialize lists to store energy and differential entropy for each channel\n",
    "        channel_theta_energy_values = []\n",
    "        channel_alpha_energy_values = []\n",
    "        channel_beta_energy_values = []\n",
    "        channel_gamma_energy_values = []\n",
    "\n",
    "        channel_theta_diff_entropy_values = []\n",
    "        channel_alpha_diff_entropy_values = []\n",
    "        channel_beta_diff_entropy_values = []\n",
    "        channel_gamma_diff_entropy_values = []\n",
    "\n",
    "        # Loop through each channel in the subtrial\n",
    "        for channel_data_tuple in subtrial_data:\n",
    "            # Initialize list to store flattened channel data\n",
    "            flattened_channel_data = []\n",
    "\n",
    "            # Loop through each array within the channel data tuple\n",
    "            for data_array in channel_data_tuple:\n",
    "                # If the array has more than one dimension, flatten it\n",
    "                if data_array.ndim > 1:\n",
    "                    flattened_channel_data.append(data_array.flatten())\n",
    "                elif data_array.ndim == 1:\n",
    "                    flattened_channel_data.append(data_array)  # Add 1-dimensional arrays as is\n",
    "                else:\n",
    "                    # Skip zero-dimensional arrays\n",
    "                    continue\n",
    "\n",
    "            # Concatenate flattened channel data into a single array if there's any data\n",
    "            if flattened_channel_data:\n",
    "                channel_data = np.concatenate(flattened_channel_data)\n",
    "            else:\n",
    "                # If no data was found, skip computation for this channel\n",
    "                continue\n",
    "\n",
    "            # Compute energy in different frequency bands for the channel\n",
    "            theta_energy = compute_energy(channel_data, fs)\n",
    "            alpha_energy = compute_energy(channel_data, fs)\n",
    "            beta_energy = compute_energy(channel_data, fs)\n",
    "            gamma_energy = compute_energy(channel_data, fs)\n",
    "\n",
    "            # Compute differential entropy in different frequency bands for the channel\n",
    "            theta_diff_entropy = compute_differential_entropy(channel_data, fs, theta_band)\n",
    "            alpha_diff_entropy = compute_differential_entropy(channel_data, fs, alpha_band)\n",
    "            beta_diff_entropy = compute_differential_entropy(channel_data, fs, beta_band)\n",
    "            gamma_diff_entropy = compute_differential_entropy(channel_data, fs, gamma_band)\n",
    "\n",
    "            # Append results to channel lists\n",
    "            channel_theta_energy_values.append(theta_energy)\n",
    "            channel_alpha_energy_values.append(alpha_energy)\n",
    "            channel_beta_energy_values.append(beta_energy)\n",
    "            channel_gamma_energy_values.append(gamma_energy)\n",
    "\n",
    "            channel_theta_diff_entropy_values.append(theta_diff_entropy)\n",
    "            channel_alpha_diff_entropy_values.append(alpha_diff_entropy)\n",
    "            channel_beta_diff_entropy_values.append(beta_diff_entropy)\n",
    "            channel_gamma_diff_entropy_values.append(gamma_diff_entropy)\n",
    "\n",
    "        # Append channel-wise results to trial lists\n",
    "        trial_theta_energy_values.append(channel_theta_energy_values)\n",
    "        trial_alpha_energy_values.append(channel_alpha_energy_values)\n",
    "        trial_beta_energy_values.append(channel_beta_energy_values)\n",
    "        trial_gamma_energy_values.append(channel_gamma_energy_values)\n",
    "\n",
    "        trial_theta_diff_entropy_values.append(channel_theta_diff_entropy_values)\n",
    "        trial_alpha_diff_entropy_values.append(channel_alpha_diff_entropy_values)\n",
    "        trial_beta_diff_entropy_values.append(channel_beta_diff_entropy_values)\n",
    "        trial_gamma_diff_entropy_values.append(channel_gamma_diff_entropy_values)\n",
    "\n",
    "    # Append trial-wise results to overall lists\n",
    "    theta_energy_values.append(trial_theta_energy_values)\n",
    "    alpha_energy_values.append(trial_alpha_energy_values)\n",
    "    beta_energy_values.append(trial_beta_energy_values)\n",
    "    gamma_energy_values.append(trial_gamma_energy_values)\n",
    "\n",
    "    theta_diff_entropy_values.append(trial_theta_diff_entropy_values)\n",
    "    alpha_diff_entropy_values.append(trial_alpha_diff_entropy_values)\n",
    "    beta_diff_entropy_values.append(trial_beta_diff_entropy_values)\n",
    "    gamma_diff_entropy_values.append(trial_gamma_diff_entropy_values)\n",
    "\n",
    "\n",
    "# Compute mean energy and differential entropy across trials and subtrials\n",
    "theta_energy_mean = np.mean(theta_energy_values, axis=(1, 2 ))\n",
    "alpha_energy_mean = np.mean(alpha_energy_values, axis=(1, 2))\n",
    "beta_energy_mean = np.mean(beta_energy_values, axis=(1, 2))\n",
    "gamma_energy_mean = np.mean(gamma_energy_values, axis=(1, 2))\n",
    "\n",
    "theta_diff_entropy_mean = np.mean(theta_diff_entropy_values, axis=(1, 2))\n",
    "alpha_diff_entropy_mean = np.mean(alpha_diff_entropy_values, axis=(1, 2))\n",
    "beta_diff_entropy_mean = np.mean(beta_diff_entropy_values, axis=(1, 2))\n",
    "gamma_diff_entropy_mean = np.mean(gamma_diff_entropy_values, axis=(1, 2))\n",
    "\n",
    "# Print shapes after computing mean\n",
    "print(\"\\nShapes after computing mean:\")\n",
    "print(\"theta_energy_mean shape:\", theta_energy_mean.shape)\n",
    "print(\"alpha_energy_mean shape:\", alpha_energy_mean.shape)\n",
    "print(\"beta_energy_mean shape:\", beta_energy_mean.shape)\n",
    "print(\"gamma_energy_mean shape:\", gamma_energy_mean.shape)\n",
    "print(\"theta_diff_entropy_mean shape:\", theta_diff_entropy_mean.shape)\n",
    "print(\"alpha_diff_entropy_mean shape:\", alpha_diff_entropy_mean.shape)\n",
    "print(\"beta_diff_entropy_mean shape:\", beta_diff_entropy_mean.shape)\n",
    "print(\"gamma_diff_entropy_mean shape:\", gamma_diff_entropy_mean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean values for each trial\n",
    "for i in range(len(theta_energy_mean)):\n",
    "    print(f\"Trial {i+1}:\")\n",
    "    print(f\"\\tTheta Energy: {theta_energy_mean[i]}\")\n",
    "    print(f\"\\tAlpha Energy: {alpha_energy_mean[i]}\")\n",
    "    print(f\"\\tBeta Energy: {beta_energy_mean[i]}\")\n",
    "    print(f\"\\tGamma Energy: {gamma_energy_mean[i]}\")\n",
    "    print(f\"\\tTheta Diff. Entropy: {theta_diff_entropy_mean[i]}\")\n",
    "    print(f\"\\tAlpha Diff. Entropy: {alpha_diff_entropy_mean[i]}\")\n",
    "    print(f\"\\tBeta Diff. Entropy: {beta_diff_entropy_mean[i]}\")\n",
    "    print(f\"\\tGamma Diff. Entropy: {gamma_diff_entropy_mean[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to hold the mean values\n",
    "new_data = {\n",
    "    \"Mean Theta Energy\": theta_energy_mean,\n",
    "    \"Mean Alpha Energy\": alpha_energy_mean,\n",
    "    \"Mean Beta Energy\": beta_energy_mean,\n",
    "    \"Mean Gamma Energy\": gamma_energy_mean,\n",
    "    \"Mean Theta Differential Entropy\": theta_diff_entropy_mean,\n",
    "    \"Mean Alpha Differential Entropy\": alpha_diff_entropy_mean,\n",
    "    \"Mean Beta Differential Entropy\": beta_diff_entropy_mean,\n",
    "    \"Mean Gamma Differential Entropy\": gamma_diff_entropy_mean\n",
    "}\n",
    "\n",
    "# Update the existing dictionary with new features\n",
    "data.update(new_data)\n",
    "\n",
    "# Create a DataFrame from the updated dictionary\n",
    "outcome_df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "outcome_df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Wavelet features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 128  # Sampling rate of the EEG signal\n",
    "\n",
    "def compute_features(signal):\n",
    "    \n",
    "    # Perform wavelet decomposition\n",
    "    coeffs = pywt.wavedec(signal, wavelet='db4', level=5)\n",
    "\n",
    "    # Initialize list to store features\n",
    "    features = []\n",
    "\n",
    "    # Extract statistical features from wavelet coefficients\n",
    "    for coeff in coeffs:\n",
    "        features.extend([np.mean(coeff), np.var(coeff), np.std(coeff), np.max(coeff), np.min(coeff)])\n",
    "\n",
    "    return features\n",
    "\n",
    "# Initialize lists to store features for each trial\n",
    "wavelet_features_per_trial = []\n",
    "t=0\n",
    "# Loop through each trial\n",
    "for trial_subtrials in first_participant_subtrials:\n",
    "    print(t)\n",
    "    t+=1\n",
    "    # Initialize lists to store features for each subtrial within the trial\n",
    "    wavelet_features_per_subtrial = []\n",
    "\n",
    "    # Loop through each subtrial in the trial\n",
    "    for subtrial_data in trial_subtrials:\n",
    "        # Initialize lists to store features for each channel within the subtrial\n",
    "        wavelet_features_per_channel = []\n",
    "\n",
    "        # Loop through each channel in the subtrial\n",
    "        for channel_data_tuple in subtrial_data:\n",
    "            # Initialize an empty list to store flattened channel data\n",
    "            flattened_channel_data = []\n",
    "\n",
    "            # Loop through each array within the channel data tuple\n",
    "            for data_array in channel_data_tuple:\n",
    "                # If the array has more than one dimension, flatten it\n",
    "                if data_array.ndim > 1:\n",
    "                    flattened_channel_data.append(data_array.flatten())\n",
    "                else:\n",
    "                    flattened_channel_data.append(data_array)\n",
    "\n",
    "            # Concatenate the flattened channel data into a single one-dimensional array\n",
    "            channel_data = np.concatenate(flattened_channel_data)\n",
    "\n",
    "            # Compute features for the channel data\n",
    "            features = compute_features(channel_data)\n",
    "\n",
    "            # Append features to the list\n",
    "            wavelet_features_per_channel.append(features)\n",
    "\n",
    "        # Append features for all channels in the subtrial to the list\n",
    "        wavelet_features_per_subtrial.append(wavelet_features_per_channel)\n",
    "\n",
    "    # Append features for all subtrials in the trial to the list\n",
    "    wavelet_features_per_trial.append(wavelet_features_per_subtrial)\n",
    "\n",
    "# Convert list of features to NumPy array\n",
    "wavelet_features_per_trial = np.array(wavelet_features_per_trial)\n",
    "\n",
    "# Now 'wavelet_features_per_trial' contains the extracted features from the wavelet coefficients for each trial\n",
    "print(wavelet_features_per_trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelet_features_per_trial_mean =np.mean(wavelet_features_per_trial, axis=(1, 2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of wavelet_features_per_trial\n",
    "print(\"Shape of wavelet_features_per_trial:\", wavelet_features_per_trial_mean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean values for each trial\n",
    "for i in range(len(wavelet_features_per_trial_mean)):\n",
    "    print(f\"Trial {i+1}:\")\n",
    "    print(f\"\\tWavelet Features: {wavelet_features_per_trial_mean[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wavelet_data = {}\n",
    "\n",
    "# Assuming you have 30 features\n",
    "for i in range(30):\n",
    "    new_wavelet_data[f\"Mean Wavelet Feature {i+1}\"] = wavelet_features_per_trial_mean[:, i]\n",
    "\n",
    "# Update the existing dictionary with new features\n",
    "data.update(new_wavelet_data)\n",
    "\n",
    "# Create a DataFrame from the updated dictionary\n",
    "outcome_df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "outcome_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Wavelet entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 128  # Sampling rate of the EEG signal\n",
    "\n",
    "def compute_wavelet_entropy(signal):\n",
    "   \n",
    "    # Perform wavelet decomposition with level 4 and zero-padding\n",
    "    coeffs = pywt.wavedec(signal, wavelet='db4', level=4, mode='zero')\n",
    "\n",
    "    # Initialize list to store entropy values\n",
    "    entropy_values = []\n",
    "\n",
    "    # Calculate entropy for each wavelet coefficient\n",
    "    for coeff in coeffs:\n",
    "        # Compute probability distribution\n",
    "        prob_distribution = np.abs(coeff) / np.sum(np.abs(coeff))\n",
    "        \n",
    "        # Avoid division by zero by adding a small epsilon value\n",
    "        prob_distribution[prob_distribution == 0] = 1e-10\n",
    "\n",
    "        # Compute entropy\n",
    "        entropy = -np.sum(prob_distribution * np.log2(prob_distribution))\n",
    "\n",
    "        # Append entropy to the list\n",
    "        entropy_values.append(entropy)\n",
    "\n",
    "    return np.array(entropy_values)\n",
    "\n",
    "\n",
    "# Initialize lists to store entropy values for each trial\n",
    "wavelet_entropy_values_per_trial = []\n",
    "t = 0\n",
    "# Loop through each trial\n",
    "for trial_subtrials in first_participant_subtrials:\n",
    "    print(t)\n",
    "    t += 1\n",
    "    # Initialize lists to store entropy values for each subtrial within the trial\n",
    "    wavelet_entropy_values_per_subtrial = []\n",
    "\n",
    "    # Loop through each subtrial in the trial\n",
    "    for subtrial_data in trial_subtrials:\n",
    "        # Initialize lists to store entropy values for each channel within the subtrial\n",
    "        wavelet_entropy_values_per_channel = []\n",
    "\n",
    "        # Loop through each channel in the subtrial\n",
    "        for channel_data_tuple in subtrial_data:\n",
    "            # Initialize an empty list to store flattened channel data\n",
    "            flattened_channel_data = []\n",
    "\n",
    "            # Loop through each array within the channel data tuple\n",
    "            for data_array in channel_data_tuple:\n",
    "                # If the array has more than one dimension, flatten it\n",
    "                if data_array.ndim > 1:\n",
    "                    flattened_channel_data.append(data_array.flatten())\n",
    "                else:\n",
    "                    flattened_channel_data.append(data_array)\n",
    "\n",
    "            # Concatenate the flattened channel data into a single one-dimensional array\n",
    "            channel_data = np.concatenate(flattened_channel_data)\n",
    "\n",
    "            # Compute wavelet entropy for the channel data\n",
    "            entropy_values = compute_wavelet_entropy(channel_data)\n",
    "\n",
    "            # Append entropy values to the list\n",
    "            wavelet_entropy_values_per_channel.append(entropy_values)\n",
    "\n",
    "        # Append entropy values for all channels in the subtrial to the list\n",
    "        wavelet_entropy_values_per_subtrial.append(wavelet_entropy_values_per_channel)\n",
    "\n",
    "    # Append entropy values for all subtrials in the trial to the list\n",
    "    wavelet_entropy_values_per_trial.append(wavelet_entropy_values_per_subtrial)\n",
    "\n",
    "# Convert list of entropy values to NumPy array\n",
    "wavelet_entropy_values_per_trial = np.array(wavelet_entropy_values_per_trial)\n",
    "\n",
    "# Now 'wavelet_entropy_values_per_trial' contains the computed wavelet entropy values for each trial\n",
    "print(wavelet_entropy_values_per_trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of wavelet_features_per_trial\n",
    "print(\"Shape of wavelet_features_per_trial:\", wavelet_entropy_values_per_trial.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelet_entropy_values_per_trial_mean =np.mean(wavelet_entropy_values_per_trial, axis=(1, 2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of wavelet_features_per_trial\n",
    "print(\"Shape of wavelet_features_per_trial:\", wavelet_entropy_values_per_trial_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean values for each trial\n",
    "for i in range(len(wavelet_entropy_values_per_trial_mean)):\n",
    "    print(f\"Trial {i+1}:\")\n",
    "    print(f\"\\tWavelet Features: {wavelet_entropy_values_per_trial_mean[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wavelet_data = {}\n",
    "\n",
    "# Assuming you have 30 features\n",
    "for i in range(5):\n",
    "    new_wavelet_data[f\"Mean Entropy Value {i+1}\"] = wavelet_entropy_values_per_trial_mean[:, i]\n",
    "\n",
    "# Update the existing dictionary with new features\n",
    "data.update(new_wavelet_data)\n",
    "\n",
    "# Create a DataFrame from the updated dictionary\n",
    "outcome_df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "outcome_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auto_correlation(signal):\n",
    "    # Compute auto-correlation\n",
    "    auto_corr = np.correlate(signal, signal, mode='full')\n",
    "    # Extract relevant features from auto-correlation\n",
    "    features = [\n",
    "        np.mean(auto_corr),\n",
    "        np.max(auto_corr),\n",
    "        np.min(auto_corr),\n",
    "        np.std(auto_corr)\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def compute_zero_crossing_rate(signal):\n",
    "    # Count zero crossings\n",
    "    zero_crossings = np.where(np.diff(np.sign(signal)))[0]\n",
    "    # Compute zero crossing rate\n",
    "    zero_crossing_rate = len(zero_crossings) / len(signal)\n",
    "    return [zero_crossing_rate]\n",
    "\n",
    "# Initialize lists to store mean features for each trial\n",
    "mean_auto_corr_features_per_trial = []\n",
    "mean_zero_crossing_rate_features_per_trial = []\n",
    "t=0\n",
    "\n",
    "# Loop through each trial\n",
    "for trial_subtrials in first_participant_subtrials:\n",
    "    auto_corr_features = []\n",
    "    zero_crossing_rate_features = []\n",
    "    t+=1\n",
    "    print(t)\n",
    "\n",
    "    # Loop through each subtrial in the trial\n",
    "    for subtrial_data in trial_subtrials:\n",
    "        # Loop through each channel in the subtrial\n",
    "        for channel_data_tuple in subtrial_data:\n",
    "            # Initialize list to store flattened channel data\n",
    "            flattened_channel_data = []\n",
    "\n",
    "            # Loop through each array within the channel data tuple\n",
    "            for data_array in channel_data_tuple:\n",
    "                # If the array has more than one dimension, flatten it\n",
    "                if data_array.ndim > 1:\n",
    "                    flattened_channel_data.append(data_array.flatten())\n",
    "                elif data_array.ndim == 1:\n",
    "                    flattened_channel_data.append(data_array)  # Add 1-dimensional arrays as is\n",
    "                else:\n",
    "                    # Skip zero-dimensional arrays\n",
    "                    continue\n",
    "\n",
    "            # Concatenate flattened channel data into a single array if there's any data\n",
    "            if flattened_channel_data:\n",
    "                channel_data = np.concatenate(flattened_channel_data)\n",
    "            else:\n",
    "                # If no data was found, skip computation for this channel\n",
    "                continue\n",
    "\n",
    "            # Compute auto-correlation features for the channel data\n",
    "            auto_corr_features.append(compute_auto_correlation(channel_data))\n",
    "\n",
    "            # Compute zero crossing rate features for the channel data\n",
    "            zero_crossing_rate_features.append(compute_zero_crossing_rate(channel_data))\n",
    "\n",
    "    # Convert lists of features to NumPy arrays for easier mean calculation\n",
    "    auto_corr_features = np.array(auto_corr_features)\n",
    "    zero_crossing_rate_features = np.array(zero_crossing_rate_features)\n",
    "\n",
    "    # Compute mean features for the current trial\n",
    "    mean_auto_corr_features = np.mean(auto_corr_features, axis=0)\n",
    "    mean_zero_crossing_rate_features = np.mean(zero_crossing_rate_features, axis=0)\n",
    "\n",
    "    # Store the mean features\n",
    "    mean_auto_corr_features_per_trial.append(mean_auto_corr_features)\n",
    "    mean_zero_crossing_rate_features_per_trial.append(mean_zero_crossing_rate_features)\n",
    "\n",
    "# Convert lists of mean features to NumPy arrays\n",
    "mean_auto_corr_features_per_trial = np.array(mean_auto_corr_features_per_trial)\n",
    "mean_zero_crossing_rate_features_per_trial = np.array(mean_zero_crossing_rate_features_per_trial)\n",
    "\n",
    "# Print mean features for each trial\n",
    "print(\"Mean Auto-correlation Features per Trial:\", mean_auto_corr_features_per_trial)\n",
    "print(\"Mean Zero Crossing Rate Features per Trial:\", mean_zero_crossing_rate_features_per_trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {}\n",
    "\n",
    "# Assuming you have 30 features\n",
    "for i in range(4):\n",
    "    new_data[f\"Mean Auto-correlation feature {i+1}\"] = mean_auto_corr_features_per_trial[:, i]\n",
    "\n",
    "# Update the existing dictionary with new features\n",
    "data.update(new_data)\n",
    "\n",
    "# Create a DataFrame from the updated dictionary\n",
    "merged_data = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 2D array to a 1D array\n",
    "flattened_zero_crossing_rate_features = mean_zero_crossing_rate_features_per_trial.flatten()\n",
    "\n",
    "# Add the flattened array to the dictionary\n",
    "new_data = {\n",
    "    \"Zero Crossing Rate\": flattened_zero_crossing_rate_features\n",
    "}\n",
    "\n",
    "# Update the existing dictionary with new features\n",
    "data.update(new_data)\n",
    "\n",
    "# Create a DataFrame from the updated dictionary\n",
    "outcome_df = pd.DataFrame(data)\n",
    "\n",
    "outcome_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASSIFICATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Participant_id  Experiment_id      Activity  Mobility  Complexity  \\\n",
      "0              1              1  2.551833e-11  0.656182    1.562014   \n",
      "1              1              2  2.850130e-11  0.641845    1.566043   \n",
      "2              1              3  2.687303e-11  0.647403    1.563928   \n",
      "3              1              4  2.338460e-11  0.714100    1.570787   \n",
      "4              1              5  2.334399e-11  0.723316    1.570053   \n",
      "\n",
      "           Mean  Standard_Deviation   Maximum   Minimum  Root_Mean_Square  \\\n",
      "0  5.341950e-24            0.000005  0.000019 -0.000020          0.000005   \n",
      "1 -9.487526e-24            0.000005  0.000021 -0.000022          0.000005   \n",
      "2 -4.103842e-25            0.000005  0.000021 -0.000021          0.000005   \n",
      "3  2.643152e-24            0.000005  0.000023 -0.000023          0.000005   \n",
      "4 -1.905852e-24            0.000005  0.000023 -0.000023          0.000005   \n",
      "\n",
      "   ...  Mean Entropy Value 1  Mean Entropy Value 2  Mean Entropy Value 3  \\\n",
      "0  ...              2.842203              1.735181              1.917658   \n",
      "1  ...              2.842203              1.735231              1.917768   \n",
      "2  ...              2.842203              1.735194              1.917720   \n",
      "3  ...              2.842205              1.735103              1.917356   \n",
      "4  ...              2.842205              1.735117              1.917345   \n",
      "\n",
      "   Mean Entropy Value 4  Mean Entropy Value 5  Zero Crossing Rate  Valence  \\\n",
      "0              1.822013              1.151723            0.111623     7.71   \n",
      "1              1.822072              1.151727            0.109340     8.10   \n",
      "2              1.822036              1.151723            0.110297     8.58   \n",
      "3              1.822009              1.152042            0.120338     4.94   \n",
      "4              1.822010              1.152083            0.121442     6.96   \n",
      "\n",
      "   Arousal  Valence_class  Arousal_class  \n",
      "0     7.60              1              1  \n",
      "1     7.31              1              1  \n",
      "2     7.54              1              1  \n",
      "3     6.01              0              1  \n",
      "4     3.92              1              0  \n",
      "\n",
      "[5 rows x 61 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define a function to classify valence and arousal\n",
    "def classify_value(value):\n",
    "    return 1 if value > 5 else 0\n",
    "\n",
    "# Apply the function to create the new columns\n",
    "merged_data['Valence_class'] = merged_data['Valence'].apply(classify_value)\n",
    "merged_data['Arousal_class'] = merged_data['Arousal'].apply(classify_value)\n",
    "\n",
    "# Display the first few rows to check the new columns\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled samples: 12\n",
      "Number of unlabeled samples: 28\n",
      "Classification report for Label Spreading:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         6\n",
      "           1       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00        12\n",
      "   macro avg       1.00      1.00      1.00        12\n",
      "weighted avg       1.00      1.00      1.00        12\n",
      "\n",
      "Accuracy: 1.0\n",
      "    Participant_id  Experiment_id  Activity  Mobility  Complexity      Mean  \\\n",
      "0              0.0      -0.649722 -1.321360  0.932445    0.206650 -1.318546   \n",
      "1              0.0       1.169500 -0.913933  0.278778   -1.326372  0.466494   \n",
      "2              0.0      -0.043315 -0.397352  1.435491    1.703248  0.701949   \n",
      "3              0.0       0.303204 -0.335891  1.586976    1.649586  1.175066   \n",
      "4              0.0       0.216574  1.500910 -1.170471    0.221820  0.725495   \n",
      "5              0.0      -1.429389 -0.258081  1.765899    0.725503  0.881483   \n",
      "6              0.0      -0.996241  1.453465 -1.141908    0.718398 -1.087506   \n",
      "7              0.0       1.342759 -0.946417  0.267044   -0.489222  0.784358   \n",
      "8              0.0      -0.389833  0.958312 -0.956626   -0.777816 -0.484154   \n",
      "9              0.0      -1.256129 -1.303429  0.767470    0.452664 -1.571660   \n",
      "10             0.0      -0.303204 -0.854508  0.246518    0.609592 -0.516529   \n",
      "11             0.0       1.689278 -1.406714  1.078890    1.712892 -0.659273   \n",
      "12             0.0      -1.082870  0.817275 -0.913094   -0.517104 -0.245756   \n",
      "13             0.0       0.476463  1.302240 -1.083161    0.823101 -0.568034   \n",
      "14             0.0       1.082870 -1.310284  0.808476    0.684596 -1.573131   \n",
      "15             0.0      -1.516018  0.891963 -0.926619   -0.608848  0.235455   \n",
      "16             0.0       0.996241  0.688883 -0.745808   -1.061906  0.722551   \n",
      "17             0.0      -1.602648  1.428757 -1.150979   -0.197489 -1.684972   \n",
      "18             0.0      -1.689278  0.445355 -0.572206   -0.981297  1.452461   \n",
      "19             0.0      -0.476463 -0.850079  0.333265    1.053475  0.931517   \n",
      "20             0.0       0.909611 -1.378167  1.171371    2.044749 -0.700478   \n",
      "21             0.0      -0.909611 -1.155833  0.635354    0.586881  0.203815   \n",
      "22             0.0      -1.342759 -0.271467  2.137967    0.582816 -0.080938   \n",
      "23             0.0       0.389833  0.965525 -0.941176   -0.749409  2.010194   \n",
      "24             0.0      -1.169500  0.554543 -0.635593   -1.267361  0.749040   \n",
      "25             0.0      -0.129944  1.408278 -1.107497   -1.189090  1.402427   \n",
      "26             0.0       1.256129 -1.024319  0.517523    0.466733  0.734324   \n",
      "27             0.0       0.822981  0.037548 -0.452556   -1.396005  0.604824   \n",
      "28             0.0       1.516018  1.011627 -1.010149   -0.076317  1.555472   \n",
      "29             0.0       1.429389  1.269880 -1.052292   -0.499902 -0.442949   \n",
      "30             0.0       0.129944 -0.975090  0.424596    1.360112 -0.416460   \n",
      "31             0.0       1.602648 -1.334725  1.025878    0.610451 -1.365637   \n",
      "32             0.0       0.736352  0.274945 -0.531384   -1.307416  0.522415   \n",
      "33             0.0       0.649722  0.890529 -0.906042   -0.894130 -0.381142   \n",
      "34             0.0      -0.563093 -0.887111  0.231155   -1.488710 -0.070636   \n",
      "35             0.0       0.563093 -0.803881  0.027466   -0.551149 -0.366426   \n",
      "36             0.0      -0.822981 -0.185840 -0.343782   -1.210818 -2.005779   \n",
      "37             0.0      -0.216574 -0.261584  1.990884    1.226128  0.462080   \n",
      "38             0.0       0.043315  1.480914 -1.187966    0.147828  0.759341   \n",
      "39             0.0      -0.736352  0.795117 -0.834138   -0.996859 -1.540756   \n",
      "\n",
      "    Standard_Deviation   Maximum   Minimum  Root_Mean_Square  ...  \\\n",
      "0            -1.290909 -1.042078  1.057670         -1.290909  ...   \n",
      "1            -0.866352 -1.137448  1.532567         -0.866352  ...   \n",
      "2            -0.723307  0.738553 -0.486492         -0.723307  ...   \n",
      "3            -0.648521  0.992656 -0.843030         -0.648521  ...   \n",
      "4             1.491068  0.806211 -1.010674          1.491068  ...   \n",
      "5            -0.542565  2.548114 -1.884138         -0.542565  ...   \n",
      "6             1.447262  0.816338 -0.894489          1.447262  ...   \n",
      "7            -0.887250 -1.100329  1.018584         -0.887250  ...   \n",
      "8             1.026725  0.358578 -0.767004          1.026725  ...   \n",
      "9            -1.283354 -1.303744  1.426513         -1.283354  ...   \n",
      "10           -0.749073 -0.999032  0.746565         -0.749073  ...   \n",
      "11           -1.342850 -0.656717  0.644196         -1.342850  ...   \n",
      "12            0.877958  0.087501 -0.024447          0.877958  ...   \n",
      "13            1.304306  0.921767 -1.074189          1.304306  ...   \n",
      "14           -1.287992 -1.331219  1.078467         -1.287992  ...   \n",
      "15            0.955214  0.589843 -0.437100          0.955214  ...   \n",
      "16            0.711130  0.177213 -0.014470          0.711130  ...   \n",
      "17            1.439026  0.900601 -1.129603          1.439026  ...   \n",
      "18            0.418103 -0.440398  0.334167          0.418103  ...   \n",
      "19           -0.748781 -0.631317  0.671334         -0.748781  ...   \n",
      "20           -1.316010 -0.867230  0.802520         -1.316010  ...   \n",
      "21           -1.105048 -0.865786  0.984241         -1.105048  ...   \n",
      "22           -0.503033  2.391414 -2.168209         -0.503033  ...   \n",
      "23            0.995550  0.160218 -0.084266          0.995550  ...   \n",
      "24            0.522675 -0.456963  0.524321          0.522675  ...   \n",
      "25            1.397859  0.781928 -0.642892          1.397859  ...   \n",
      "26           -0.958751 -0.727786  1.064687         -0.958751  ...   \n",
      "27            0.084445 -0.371397  0.140287          0.084445  ...   \n",
      "28            1.064972  0.462356 -0.658495          1.064972  ...   \n",
      "29            1.236234  0.275657 -0.564190          1.236234  ...   \n",
      "30           -0.886445 -0.552213  0.971705         -0.886445  ...   \n",
      "31           -1.286794 -1.109884  0.705841         -1.286794  ...   \n",
      "32            0.301598 -0.119329 -0.315066          0.301598  ...   \n",
      "33            0.929035  0.216995  0.022156          0.929035  ...   \n",
      "34           -0.811700 -1.003248  1.380354         -0.811700  ...   \n",
      "35           -0.690990 -1.180311  1.110731         -0.690990  ...   \n",
      "36           -0.120354 -0.902385  0.723093         -0.120354  ...   \n",
      "37           -0.510857  1.994797 -2.188959         -0.510857  ...   \n",
      "38            1.509593  1.200869 -1.503851          1.509593  ...   \n",
      "39            0.848184  0.377204 -0.248436          0.848184  ...   \n",
      "\n",
      "    Mean Wavelet Feature 29  Mean Wavelet Feature 30  Mean Entropy Value 1  \\\n",
      "0              4.336809e-19             6.505213e-19              0.403564   \n",
      "1              1.301043e-18             6.505213e-19             -1.114541   \n",
      "2              1.734723e-18             3.252607e-19              0.792673   \n",
      "3              4.336809e-18            -3.252607e-19              1.475955   \n",
      "4              8.673617e-19            -7.589415e-19              1.227196   \n",
      "5              8.673617e-19             3.252607e-19              1.789537   \n",
      "6             -2.602085e-18             5.421011e-19              1.117120   \n",
      "7             -1.301043e-18            -6.505213e-19             -0.315347   \n",
      "8              2.602085e-18             1.192622e-18             -0.130497   \n",
      "9             -4.336809e-18             3.252607e-19             -0.608017   \n",
      "10            -2.602085e-18            -3.252607e-19             -0.304070   \n",
      "11            -1.301043e-18             2.168404e-19              0.050446   \n",
      "12             1.301043e-18            -1.084202e-19              0.451429   \n",
      "13             0.000000e+00             8.673617e-19              1.047784   \n",
      "14             3.035766e-18             1.084202e-19             -0.067125   \n",
      "15             0.000000e+00            -7.589415e-19             -1.144360   \n",
      "16            -2.602085e-18            -5.421011e-19             -1.772459   \n",
      "17            -6.071532e-18            -4.336809e-19             -0.600224   \n",
      "18            -4.336809e-19            -7.589415e-19             -0.981173   \n",
      "19             8.673617e-19             1.084202e-19             -0.630272   \n",
      "20            -2.168404e-18             6.505213e-19              0.478237   \n",
      "21             2.602085e-18            -5.421011e-19             -0.220574   \n",
      "22            -1.734723e-18             3.252607e-19              1.939493   \n",
      "23             4.336809e-19             3.252607e-19              0.290894   \n",
      "24             5.637851e-18             5.421011e-19             -0.611643   \n",
      "25            -8.673617e-19            -5.421011e-19              0.126118   \n",
      "26            -8.673617e-19             0.000000e+00             -0.655303   \n",
      "27            -8.673617e-19            -2.168404e-19             -0.683258   \n",
      "28            -4.336809e-19            -4.336809e-19              0.355923   \n",
      "29            -2.602085e-18            -6.505213e-19              0.892008   \n",
      "30            -2.168404e-18             4.336809e-19             -0.306890   \n",
      "31            -1.301043e-18             2.168404e-19              0.109136   \n",
      "32             1.734723e-18            -2.168404e-19             -0.085859   \n",
      "33             1.734723e-18            -1.192622e-18             -0.386968   \n",
      "34             4.770490e-18             1.084202e-19             -1.533975   \n",
      "35             4.336809e-19             0.000000e+00             -2.395934   \n",
      "36            -1.734723e-18            -5.421011e-19             -0.560898   \n",
      "37             2.168404e-18            -4.336809e-19              2.467601   \n",
      "38             4.336809e-19            -5.421011e-19              0.628475   \n",
      "39            -2.168404e-18            -1.084202e-19             -0.534204   \n",
      "\n",
      "    Mean Entropy Value 2  Mean Entropy Value 3  Mean Entropy Value 4  \\\n",
      "0              -1.164947             -1.138955             -1.154918   \n",
      "1              -0.864158             -0.399383             -0.856038   \n",
      "2              -1.097563             -1.183681              0.200311   \n",
      "3              -0.952533             -1.296707              0.088883   \n",
      "4               1.659785              1.228967              1.310621   \n",
      "5              -0.813226             -1.356943              0.306581   \n",
      "6               1.602018              1.279478              1.051451   \n",
      "7              -0.845102             -0.494382             -1.289583   \n",
      "8               0.716360              1.102583              0.567492   \n",
      "9              -0.922886             -1.109061             -1.359889   \n",
      "10             -0.872157             -0.467100             -1.021743   \n",
      "11             -1.235793             -1.252661             -0.842849   \n",
      "12              1.182447              0.998674              0.477748   \n",
      "13              1.418166              1.121021              1.490190   \n",
      "14             -0.920519             -1.073921             -1.572098   \n",
      "15              0.759483              0.987672              0.954726   \n",
      "16              0.637042              0.816408              0.591385   \n",
      "17              1.397331              1.299492              1.831736   \n",
      "18              0.540432              0.591093              0.405515   \n",
      "19             -0.939825             -0.594809             -0.776790   \n",
      "20             -1.450661             -1.325930             -1.026796   \n",
      "21             -0.969625             -0.840833             -1.543951   \n",
      "22             -0.578922             -1.426241              0.317995   \n",
      "23              0.944869              1.094615              0.961602   \n",
      "24              0.390676              0.786906              0.474683   \n",
      "25              1.547632              1.230945              1.271437   \n",
      "26             -0.707475             -0.827538             -1.443637   \n",
      "27             -0.212708              0.280387             -0.243167   \n",
      "28              0.806414              1.039571              0.170993   \n",
      "29              1.196759              1.195990              1.222510   \n",
      "30             -0.779920             -0.616045             -1.371907   \n",
      "31             -1.222646             -1.177221             -1.075751   \n",
      "32              0.345496              0.418006              0.118296   \n",
      "33              1.228676              0.987406              0.845640   \n",
      "34             -0.667633             -0.464185             -0.943689   \n",
      "35             -0.712914             -0.307534             -0.892418   \n",
      "36              0.204665              0.190950             -0.099501   \n",
      "37             -0.736777             -1.389856              0.567660   \n",
      "38              1.331519              1.224841              1.734601   \n",
      "39              0.758223              0.867981              0.552669   \n",
      "\n",
      "    Mean Entropy Value 5  Zero Crossing Rate  True Label  Predicted Label  \n",
      "0               0.112855            0.965104           0                0  \n",
      "1              -0.490055            0.255480           0                0  \n",
      "2               1.886642            1.488204           1                1  \n",
      "3               2.208186            1.506961           1                1  \n",
      "4              -0.448635           -1.176272           1                1  \n",
      "5               2.677022            1.644509           0                0  \n",
      "6              -0.370871           -1.130422           0                0  \n",
      "7              -0.455150            0.245060           0                0  \n",
      "8              -0.407332           -1.003816           1                1  \n",
      "9              -0.156976            0.801505           1                1  \n",
      "10             -0.306551            0.257564           0                0  \n",
      "11              0.312228            1.269898           1                1  \n",
      "12             -0.632445           -0.947546          -1                1  \n",
      "13             -0.517863           -1.065816          -1                1  \n",
      "14              0.055972            0.927591          -1                1  \n",
      "15             -0.557042           -0.903780          -1                1  \n",
      "16             -0.574210           -0.812603          -1                1  \n",
      "17             -0.516442           -1.146574          -1                1  \n",
      "18             -0.560106           -0.567204          -1                1  \n",
      "19             -0.335945            0.409701          -1                0  \n",
      "20              0.717251            1.345967          -1                1  \n",
      "21             -0.225475            0.653536          -1                0  \n",
      "22              3.094576            1.924816          -1                0  \n",
      "23             -0.548508           -1.005379          -1                1  \n",
      "24             -0.576485           -0.736534          -1                1  \n",
      "25             -0.377162           -1.147616          -1                1  \n",
      "26             -0.254554            0.494106          -1                0  \n",
      "27             -0.622049           -0.416630          -1                1  \n",
      "28             -0.582368           -0.989227          -1                1  \n",
      "29             -0.527527           -1.056438          -1                1  \n",
      "30             -0.136140            0.389381          -1                0  \n",
      "31              0.271213            1.144334          -1                1  \n",
      "32             -0.512115           -0.520834          -1                1  \n",
      "33             -0.528396           -0.953277          -1                1  \n",
      "34             -0.534692            0.173681          -1                0  \n",
      "35             -0.552368            0.154924          -1                0  \n",
      "36             -0.644583           -0.330142          -1                0  \n",
      "37              2.654340            1.849268          -1                0  \n",
      "38             -0.489958           -1.159599          -1                1  \n",
      "39             -0.548281           -0.831880          -1                1  \n",
      "\n",
      "[40 rows x 59 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assuming `merged_data` is already defined and contains the dataset\n",
    "# Separate features and target\n",
    "X = merged_data.drop(['Valence', 'Arousal', 'Valence_class', 'Arousal_class'], axis=1)\n",
    "y = merged_data['Valence_class']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into labeled and unlabeled data\n",
    "# For this example, let's assume we use 30% of the data as labeled\n",
    "X_labeled, X_unlabeled, y_labeled, y_unlabeled = train_test_split(X, y, test_size=0.7, random_state=42, stratify=y)\n",
    "\n",
    "# Replace a portion of the labels with -1 to indicate unlabeled data\n",
    "y_unlabeled[:] = -1\n",
    "\n",
    "# Combine the labeled and unlabeled datasets\n",
    "X_combined = np.vstack((X_labeled, X_unlabeled))\n",
    "y_combined = np.concatenate((y_labeled, y_unlabeled))\n",
    "\n",
    "print(f\"Number of labeled samples: {len(y_labeled)}\")\n",
    "print(f\"Number of unlabeled samples: {len(y_unlabeled)}\")\n",
    "\n",
    "# Initialize and fit the Label Spreading model\n",
    "label_spread = LabelSpreading(kernel='rbf', alpha=0.2)\n",
    "label_spread.fit(X_combined, y_combined)\n",
    "\n",
    "# Predict the labels for all data\n",
    "y_transduced = label_spread.transduction_\n",
    "\n",
    "# Extract predictions for the labeled data\n",
    "y_pred_labeled = y_transduced[:len(y_labeled)]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification report for Label Spreading:\")\n",
    "print(classification_report(y_labeled, y_pred_labeled))\n",
    "print(\"Accuracy:\", accuracy_score(y_labeled, y_pred_labeled))\n",
    "\n",
    "# Create a DataFrame to visualize the results\n",
    "columns = merged_data.drop(['Valence', 'Arousal', 'Valence_class', 'Arousal_class'], axis=1).columns\n",
    "df = pd.DataFrame(X_combined, columns=columns)\n",
    "df['True Label'] = y_combined\n",
    "df['Predicted Label'] = y_transduced\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.25  0.125 0.25  0.125 0.   ]\n",
      "Mean accuracy: 0.15\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(label_spread, X_combined, y_combined, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.625 0.75  0.75  0.875 0.5  ]\n",
      "Mean accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_classifier = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(svm_classifier, X, y, cv=5)\n",
    "\n",
    "# Print the cross-validation scores and mean accuracy\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean accuracy:\", cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Random Forest): [0.625 0.75  0.75  0.625 0.5  ]\n",
      "Mean accuracy (Random Forest): 0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_rf = cross_val_score(rf_classifier, X, y, cv=5)\n",
    "\n",
    "# Print the cross-validation scores and mean accuracy\n",
    "print(\"Cross-validation scores (Random Forest):\", cv_scores_rf)\n",
    "print(\"Mean accuracy (Random Forest):\", cv_scores_rf.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (KNN): [0.625 0.75  0.625 0.875 0.5  ]\n",
      "Mean accuracy (KNN): 0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Odysseas\\git\\Emotion_Recognition\\ml\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\Odysseas\\git\\Emotion_Recognition\\ml\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_knn = cross_val_score(knn_classifier, X, y, cv=5)\n",
    "\n",
    "# Print the cross-validation scores and mean accuracy\n",
    "print(\"Cross-validation scores (KNN):\", cv_scores_knn)\n",
    "print(\"Mean accuracy (KNN):\", cv_scores_knn.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (XGBoost): [0.625 0.5   0.5   0.625 0.625]\n",
      "Mean accuracy (XGBoost): 0.575\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_xgb = cross_val_score(xgb_classifier, X, y, cv=5)\n",
    "\n",
    "# Print the cross-validation scores and mean accuracy\n",
    "print(\"Cross-validation scores (XGBoost):\", cv_scores_xgb)\n",
    "print(\"Mean accuracy (XGBoost):\", cv_scores_xgb.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (GBM): [0.25  0.625 0.625 0.75  0.75 ]\n",
      "Mean accuracy (GBM): 0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize the GBM classifier\n",
    "gbm_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_gbm = cross_val_score(gbm_classifier, X, y, cv=5)\n",
    "\n",
    "# Print the cross-validation scores and mean accuracy\n",
    "print(\"Cross-validation scores (GBM):\", cv_scores_gbm)\n",
    "print(\"Mean accuracy (GBM):\", cv_scores_gbm.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Logistic Regression): [0.75  0.375 1.    0.75  0.625]\n",
      "Mean accuracy (Logistic Regression): 0.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "log_reg_classifier = LogisticRegression()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_log_reg = cross_val_score(log_reg_classifier, X, y, cv=5)\n",
    "\n",
    "# Print the cross-validation scores and mean accuracy\n",
    "print(\"Cross-validation scores (Logistic Regression):\", cv_scores_log_reg)\n",
    "print(\"Mean accuracy (Logistic Regression):\", cv_scores_log_reg.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Odysseas\\git\\Emotion_Recognition\\ml\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5312 - loss: 0.6912 - val_accuracy: 0.6250 - val_loss: 0.6735\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.7188 - loss: 0.6192 - val_accuracy: 0.6250 - val_loss: 0.6845\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.7188 - loss: 0.5918 - val_accuracy: 0.6250 - val_loss: 0.7037\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7188 - loss: 0.5815 - val_accuracy: 0.6250 - val_loss: 0.7118\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.7500 - loss: 0.5733 - val_accuracy: 0.6250 - val_loss: 0.7124\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.7500 - loss: 0.5630 - val_accuracy: 0.6250 - val_loss: 0.7120\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.7500 - loss: 0.5498 - val_accuracy: 0.6250 - val_loss: 0.7144\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.7500 - loss: 0.5358 - val_accuracy: 0.6250 - val_loss: 0.7210\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.7500 - loss: 0.5234 - val_accuracy: 0.6250 - val_loss: 0.7339\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.7500 - loss: 0.5132 - val_accuracy: 0.6250 - val_loss: 0.7532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x281a17278d0>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Reshape X to mimic image data\n",
    "X_2d = X.reshape(X.shape[0], X.shape[1], 1, 1)  # Assuming X.shape[1] is the number of features\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_2d, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "X_test_scaled = scaler.transform(X_test.reshape(X_test.shape[0], -1))\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (1, 1), activation='relu', input_shape=X_train.shape[1:]),\n",
    "    Conv2D(64, (1, 1), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
